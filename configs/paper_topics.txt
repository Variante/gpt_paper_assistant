 1. New methodological improvements to self-supervised learning for better image or video representation learning related to robotics. 
    - Relevant: papers that discuss specific methods like self-supervised learning, contrastive learning, masked autoencoder, improving these methods, or analyzing them. Usually, these papers will explicitly mention self-supervised learning, representation learning, and so on.
    - Not relevant: papers about adaptation to some task. Simply following instructions or inputs are not sufficient.
 2. Shows new applications of vision language models (VLMs) in robotics like reinforcement learning or imitation learning.
    - Relevant: papers take advantage of vision language models to solve open robotics tasks like a novel task demonstrated or requested by users.
    - Not relevant: any papers that do not consider language models in robotics.
 3. Shows new network architecture, module, auxiliary loss, training method when perform reinforcement learning, imitation learning or general robotics topic.
    - Relevant: papers use sequential models or a diffusion model to learn a robot policy.
    - Not relevant: any papers introduce new robot hardwards to solve specific problems or a particular application of a certain policy learning method.
 4. Shows a significant advance in the performance of text to image, text to video, image to video diffusion models, or diffusion model for 3D model generation.
    - Relevant: papers that improve diffusion models for better generation results, papers that reduce the computational cost of a diffusion model in terms of inference and training, and papers study the image quality generated by the diffusion model.
    - Not relevant: papers about language diffusions.

 In suggesting papers to your friend, remember that he enjoys papers on self-supervised learning, policy learning, vision language models, and generative models in computer visions.
 Your friend also likes learning about surprising empirical results in language models, as well as clever statistical tricks.
 He does not want to read papers that are about primarily applications of methods to specific domains.
